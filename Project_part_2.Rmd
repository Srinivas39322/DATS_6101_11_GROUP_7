---
title: "Predicting app costs in Apple App store"
author: "Krishnasurya Gopalakrishnan, Monica Muniraj, Shashank Shivakumar, Srinivas Saiteja Tenneti"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = F, message = F)
options(scientific=T, digits = 3)

library(ezids)
library(tidyr)
library(lubridate)
library(dplyr)
library(ggplot2)
library(caret)
library(pROC)
library(randomForest)
library(dplyr)
library(corrplot)
```

## Introduction
<p>
Predicting if apps are going to be free or not -- from our eda we saw -- what all columns are useful -- what all columns we have filtered for feature selection
</p>

```{r}
df = read.csv('appleAppData.csv')
df <- na.omit(df)

df$Released_Year <- year(df$Released)
df$Updated_Year <- year(df$Updated)

df$Primary_Genre <- as.factor(df$Primary_Genre)
df$Content_Rating <- as.factor(df$Content_Rating)
df$DeveloperId <- as.factor(df$DeveloperId)
df$Released_Year <- as.factor(df$Released_Year)
df$Updated_Year <- as.factor(df$Updated_Year)

df$Size_MB <- df$Size_Bytes / (1024^2)
df$Size_Group <- cut(df$Size_MB,
                     breaks = seq(0, max(df$Size_MB), by = 50),
                     labels = seq(25, max(df$Size_MB) - 25, by = 50))

df$Primary_genre <- as.numeric(factor(df$Primary_Genre))
df$Content_rating <- as.numeric(factor(df$Content_Rating))
df$Free_Paid <- as.numeric(factor(df$Free))
df$Developer_ID <- as.numeric(df$DeveloperId)
df$Released_year <- as.numeric(df$Released_Year)
df$Updated_year <- as.numeric(df$Updated_Year)

print(str(df))
```

```{r}
# num_unique_values <- length(unique(df$DeveloperId))
# print(num_unique_values)
```

```{r}
features <- c("Primary_Genre", "Content_Rating", "Size_MB", "Average_User_Rating", "Reviews", "Released_Year", "Updated_Year")
target <- "Free"

rf_model <- randomForest(as.factor(Free) ~ ., data = df[, c(features, target)], ntree = 100)
importance <- importance(rf_model)
print(importance)
```

<p>
From the MeanDegreeGini ---, we can see that ---, ---, ---, --- columns tend to have the highest impact on the target variable and further models are built using these columns as predictors.
</p>

```{r}
selected_columns <- c("Primary_Genre", "Size_MB", "Average_User_Rating", "Released_Year", "Free")

df <- df[selected_columns]
df$Free <- as.factor(df$Free)
levels(df$Free) <- c(0, 1)

# na_counts <- colSums(is.na(df))
# print(na_counts)
str(df)
```

### Logistic Regression
<p>
Why we are choosing Logistic Regression (I think - since the nature of the target variable is binomial in nature)
</p>

```{r}
logistic_model_2 <- glm(Free ~ .,data = df, family = binomial)
# summary(logistic_model_2)
```

## Confusion Matrix for Logistic Regression
```{r}
predictions_2 <- factor(ifelse(predict(logistic_model_2, type = "response") > 0.5, 1, 0), levels = levels(df$Free))
actual_values_2 <- factor(df$Free, levels = levels(df$Free))

conf_matrix_2 <- confusionMatrix(predictions_2, actual_values_2)
print(conf_matrix_2)
```
<p>
From the above model metrics we can see that, although the model's accuracy is over 90% and the specificity being over 99%
</p>
## Distribution of Target variable
```{r}
free_apps <- df[df$Free == 1, ]
paid_apps <- df[df$Free == 0, ]

print(nrow(free_apps))
print(nrow(paid_apps))
```

<p>
From the above result we can see that there is a huge disparity between the two levels in the target variable. Almost a 11:1 ratio. Models trained on this data will result in bias in their predictions. 
</p>

```{r}
free_apps_sample <- free_apps %>% sample_n(102502)
print(nrow(free_apps_sample))
```

```{r}
merged_df <- rbind(free_apps_sample, paid_apps)
str(merged_df)
```

```{r}
print(head(merged_df))
merged_df <- merged_df[sample(1:205004), ]
print(head(merged_df))
```

```{r}
merged_df_free_apps <- df[df$Free == 1, ]
merged_df_paid_apps <- df[df$Free == 0, ]

print(nrow(merged_df_free_apps))
print(nrow(merged_df_paid_apps))
```
```{r}
logistic_model <- glm(Free ~ ., data = merged_df, family = binomial)
summary(logistic_model)
```

```{r}
predictions <- factor(ifelse(predict(logistic_model, type = "response") > 0.5, 1, 0), levels = levels(merged_df$Free))
actual_values <- factor(merged_df$Free, levels = levels(merged_df$Free))

conf_matrix <- confusionMatrix(predictions, actual_values)
print(conf_matrix)
```
# Random Forest

```{r}
selected_columns_3 <- c("Size_MB", "Average_User_Rating", "Released_Year", "Free")
dff <- df[selected_columns_3]
colnames(dff)
nrow(dff)
```

```{r}
# Assuming df is your data frame with the dependent variable 'Free' and other predictor variables

# Install and load the randomForest package if not already installed
# install.packages("randomForest")
library(randomForest)

# Set a seed for reproducibility
set.seed(123)

library(doParallel)
cl <- makeCluster(detectCores())
registerDoParallel(cl)

rf_model <- randomForest(Free ~ ., data = merged_df, ntree = 500, mtry = sqrt(ncol(merged_df) - 1), ncores = detectCores())

```

```{r}
summary(rf_model)
```

```{r}
# Assuming 'your_test_data' is your testing dataset
# predictions_rf <- factor(ifelse(predict(rf_model, newdata = merged_df, type = "response") > 0.5, 1, 0), levels = levels(merged_df$Free))
predictions_rf <- predict(rf_model, newdata = merged_df, type = "response")
results_rf <- data.frame(Actual = merged_df$Free, Predicted = predictions_rf)
library(caret)

# Create a confusion matrix
confusion_matrix_rf <- confusionMatrix(table(results_rf$Predicted, results_rf$Actual))
print(confusion_matrix_rf)
```
## Conclusion

<p>

1. **Recognizing the Impact of Skewed Data**: In real-world data analysis, skewness in data is a common occurrence. This skewness, if not addressed, can lead to the development of predictive models that fail to accurately capture the underlying patterns and trends. Recognizing and addressing this skewness is therefore essential in ensuring the validity of our models.

2. **Evaluating Model Performance on Different Data Sets**: Models trained on unbalanced data may exhibit high accuracy, but this metric can be deceptive. Such models might only be proficient in predicting the outcome for the over-represented class while neglecting the under-represented one. Conversely, models trained on balanced data, while potentially showing a slight decrease in overall accuracy, offer a more honest representation of their predictive capabilities across all classes.

3. **The Crucial Role of Data Balancing in Modeling**: The practice of balancing data, whether through resampling techniques or other methods, is not just a one-time adjustment but a repeated necessity throughout the model training process. By continually addressing the skewness in data, we enhance the model's reliability and accuracy, making it a more robust tool for predictions.

4. **Prioritizing Reliability Over Raw Accuracy**: A model that has been trained on balanced data may show a lower accuracy rate compared to its unbalanced counterpart. However, such a model is generally more reliable and offers a true reflection of its performance. This approach emphasizes the importance of a model's ability to predict outcomes accurately across all categories, rather than achieving a high accuracy rate on a skewed dataset.

5. **The Iterative Nature of Model Improvement**: The process of data balancing and model retraining is iterative, emphasizing the importance of continuous refinement. Each iteration aims to improve the model's ability to make accurate and dependable predictions. This ongoing process reflects the dynamic nature of data science and the constant need for adaptation to achieve the most accurate and reliable results.

In conclusion, these insights underscore the importance of understanding and addressing data skewness in predictive modeling. By prioritizing balanced data and reliability over raw accuracy, and committing to an iterative process of improvement, we can develop more effective and trustworthy predictive models.

</p>
